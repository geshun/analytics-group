{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Intro to Programming for Data Science\n",
        "\n",
        "geshun"
      ],
      "id": "636604c0-09e0-4352-9ccb-9fea95d91548"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reticulate::use_condaenv(\"base\")"
      ],
      "id": "0979a695-4016-4d25-b0e7-4b43ef793c85"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Import pandas\n",
        "\n",
        "Include `numpy` and `matplotlib`, we may use them down the line."
      ],
      "id": "de8506b4-ffb7-44f7-86cd-3644cc3e4bba"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "id": "b73f005d-6bcd-49ce-a786-acee4adc3426"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Open and read files using vanilla python\n",
        "\n",
        "> File name is `alpha-12-Sep-2022 21-28-50.csv`"
      ],
      "id": "ace2879e-ab61-4e03-b755-164e7a793728"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'transaction_id,transaction_date,transaction_type,transaction_category,transaction_item,transaction_total_cost,transaction_quantity,transaction_unit_price,transaction_source,transaction_measure_unit,transaction_remark\\n0000018.2020410.SS.Gen.120,2020-04-10,Sell Services,Equipment (or Tool),Generator, 120 ,6, 20 ,Other,Hourly,\\n0000017.2020225.BG.Gra.120,2020-02-25,Buy Goods,Herbicide (or Weedicide),Gramoxone, 120 ,1, 120 ,uuni0004v   Unity Agro Chemicals,Bottle,\\n0000016.2020525.SG.Oth.500,2020-05-25,Sell Goods,Inserticide (or Pesticide),Other, 500 ,5, 100 ,Other,Bottle,\\n0000015.2020525.BG.cchChiPep.300,2020-05-25,Buy Goods,Seedling (or Plant Material),cchi0004p   Chili Pepper, 300 ,3, 100 ,aalp0001v   Alpha Ventures,Sachet,\\n0000014.2020525.BG.Pet.200,2020-05-25,Buy Goods,Fuel (or Oil),Petro, 200 ,10, 20 ,Other,Gallon,Getting expensive\\n0000013.2020524.BS.Goo.200,2020-05-24,Buy Services,Other,Goodwill, 200 ,1, 200 ,Other,Other,Goodwill (gift) to a friend of the farm\\n0000012.2020316.BG.bbeBelPepGre.220,2020-03-16,Buy Goods,Seedling (or Plant Material),bbel0002p   Bell Pepper Green, 220 ,1, 220 ,ggre0002v   Green Farms,Tin,\\n0000010.2020515.BS.kstKofStoSmi.600,2020-05-15,Buy Services,Employee,ksto0001e   Kofi Stone Smith, 600 ,,  ,Truthii,Monthly,\\n0000009.2020524.BG.Ban.50,2020-05-24,Buy Goods,Other,Banana, 50 ,,  ,aalp0001v   Alpha Ventures,,\\n0000008.2020515.SG.ccaCabGre.400,2020-05-15,Sell Goods,Seedling (or Plant Material),ccab0001p   Cabbage Green, 400 ,,  ,vveg000002c   Veggie Center,,\\n0000007.2020515.SG.Oth.1000,2020-05-15,Sell Goods,Herbicide (or Weedicide),Other,\" 1,000 \",,  ,ffoo000001c   Food House,,\\n0000006.2020515.SS.Oth.200,2020-05-15,Sell Services,Equipment (or Tool),Other, 200 ,,  ,ffoo000001c   Food House,,\\n0000005.2020515.SG.Oth.500,2020-05-15,Sell Goods,Crop (or Harvest),Other, 500 ,10,  ,vveg000002c   Veggie Center,Bag,\\n0000005.2020115.BG.Oth.1000,2020-01-15,Buy Goods,Equipment (or Tool),Other,\" 1,000 \",,  ,aalp0001v   Alpha Ventures,,\\n0000004.2020515.BS.Oth.600,2020-05-15,Buy Services,Labour,Other, 600 ,,  ,aalp0001v   Alpha Ventures,,\\n0000003.2020415.BG.Oth.200,2020-04-15,Buy Goods,Fertilizer (or Manure),Other, 200 ,2, 100 ,Other,,\\n0000002.2020515.BG.Pet.100,2020-05-15,Buy Goods,Fuel (or Oil),Petro, 100 ,,  ,aalp0001v   Alpha Ventures,,\\n0000001.2020426.SG.bbeBelPepGre.500,2020-04-26,Sell Goods,Crop (or Harvest),bbel0002p   Bell Pepper Green, 500 ,1, 500 ,ssis000003c   Sister Ama Joint,Bag,\\n,,,,,,,,,,\\n,,,,,,,,,,\\n,,,,,,,,,,\\n,,,,,,,,,,\\n,,,,,,,,,,\\n,,,,,,,,,,\\n,,,,,,,,,,\\n,,,,,,,,,,\\n,,,,,,,,,,\\n,,,,,,,,,,\\n,,,,,,,,,,\\n,,,,,,,,,,\\n,,,,,,,,,,\\n,,,,,,,,,,\\n,,,,,,,,,,\\n'"
          ]
        }
      ],
      "source": [
        "with open(\"alpha-12-Sep-2022 21-28-50.csv\", \"r\") as f:\n",
        "  transaction = f.read()\n",
        "\n",
        "transaction"
      ],
      "id": "e2ab23e7-610f-4712-8d17-6894a5421ec0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that a lot of work needs to be done on `transaction` string\n",
        "object if we want to get it into tabular/rectangular form or preprocess\n",
        "so that numeric fields are actually numeric.\n",
        "\n",
        "> We can attempt to read the `.csv` file using the `csv` package which\n",
        "> comes with standard python."
      ],
      "id": "ae8b8f40-aa92-40f1-aeb5-4cc8ccf68ef1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['transaction_id', 'transaction_date', 'transaction_type', 'transaction_category', 'transaction_item', 'transaction_total_cost', 'transaction_quantity', 'transaction_unit_price', 'transaction_source', 'transaction_measure_unit', 'transaction_remark']\n",
            "['0000018.2020410.SS.Gen.120', '2020-04-10', 'Sell Services', 'Equipment (or Tool)', 'Generator', ' 120 ', '6', ' 20 ', 'Other', 'Hourly', '']\n",
            "['0000017.2020225.BG.Gra.120', '2020-02-25', 'Buy Goods', 'Herbicide (or Weedicide)', 'Gramoxone', ' 120 ', '1', ' 120 ', 'uuni0004v   Unity Agro Chemicals', 'Bottle', '']\n",
            "['0000016.2020525.SG.Oth.500', '2020-05-25', 'Sell Goods', 'Inserticide (or Pesticide)', 'Other', ' 500 ', '5', ' 100 ', 'Other', 'Bottle', '']\n",
            "['0000015.2020525.BG.cchChiPep.300', '2020-05-25', 'Buy Goods', 'Seedling (or Plant Material)', 'cchi0004p   Chili Pepper', ' 300 ', '3', ' 100 ', 'aalp0001v   Alpha Ventures', 'Sachet', '']\n",
            "['0000014.2020525.BG.Pet.200', '2020-05-25', 'Buy Goods', 'Fuel (or Oil)', 'Petro', ' 200 ', '10', ' 20 ', 'Other', 'Gallon', 'Getting expensive']\n",
            "['0000013.2020524.BS.Goo.200', '2020-05-24', 'Buy Services', 'Other', 'Goodwill', ' 200 ', '1', ' 200 ', 'Other', 'Other', 'Goodwill (gift) to a friend of the farm']\n",
            "['0000012.2020316.BG.bbeBelPepGre.220', '2020-03-16', 'Buy Goods', 'Seedling (or Plant Material)', 'bbel0002p   Bell Pepper Green', ' 220 ', '1', ' 220 ', 'ggre0002v   Green Farms', 'Tin', '']\n",
            "['0000010.2020515.BS.kstKofStoSmi.600', '2020-05-15', 'Buy Services', 'Employee', 'ksto0001e   Kofi Stone Smith', ' 600 ', '', '  ', 'Truthii', 'Monthly', '']\n",
            "['0000009.2020524.BG.Ban.50', '2020-05-24', 'Buy Goods', 'Other', 'Banana', ' 50 ', '', '  ', 'aalp0001v   Alpha Ventures', '', '']\n",
            "['0000008.2020515.SG.ccaCabGre.400', '2020-05-15', 'Sell Goods', 'Seedling (or Plant Material)', 'ccab0001p   Cabbage Green', ' 400 ', '', '  ', 'vveg000002c   Veggie Center', '', '']\n",
            "['0000007.2020515.SG.Oth.1000', '2020-05-15', 'Sell Goods', 'Herbicide (or Weedicide)', 'Other', ' 1,000 ', '', '  ', 'ffoo000001c   Food House', '', '']\n",
            "['0000006.2020515.SS.Oth.200', '2020-05-15', 'Sell Services', 'Equipment (or Tool)', 'Other', ' 200 ', '', '  ', 'ffoo000001c   Food House', '', '']\n",
            "['0000005.2020515.SG.Oth.500', '2020-05-15', 'Sell Goods', 'Crop (or Harvest)', 'Other', ' 500 ', '10', '  ', 'vveg000002c   Veggie Center', 'Bag', '']\n",
            "['0000005.2020115.BG.Oth.1000', '2020-01-15', 'Buy Goods', 'Equipment (or Tool)', 'Other', ' 1,000 ', '', '  ', 'aalp0001v   Alpha Ventures', '', '']\n",
            "['0000004.2020515.BS.Oth.600', '2020-05-15', 'Buy Services', 'Labour', 'Other', ' 600 ', '', '  ', 'aalp0001v   Alpha Ventures', '', '']\n",
            "['0000003.2020415.BG.Oth.200', '2020-04-15', 'Buy Goods', 'Fertilizer (or Manure)', 'Other', ' 200 ', '2', ' 100 ', 'Other', '', '']\n",
            "['0000002.2020515.BG.Pet.100', '2020-05-15', 'Buy Goods', 'Fuel (or Oil)', 'Petro', ' 100 ', '', '  ', 'aalp0001v   Alpha Ventures', '', '']\n",
            "['0000001.2020426.SG.bbeBelPepGre.500', '2020-04-26', 'Sell Goods', 'Crop (or Harvest)', 'bbel0002p   Bell Pepper Green', ' 500 ', '1', ' 500 ', 'ssis000003c   Sister Ama Joint', 'Bag', '']\n",
            "['', '', '', '', '', '', '', '', '', '', '']\n",
            "['', '', '', '', '', '', '', '', '', '', '']\n",
            "['', '', '', '', '', '', '', '', '', '', '']\n",
            "['', '', '', '', '', '', '', '', '', '', '']\n",
            "['', '', '', '', '', '', '', '', '', '', '']\n",
            "['', '', '', '', '', '', '', '', '', '', '']\n",
            "['', '', '', '', '', '', '', '', '', '', '']\n",
            "['', '', '', '', '', '', '', '', '', '', '']\n",
            "['', '', '', '', '', '', '', '', '', '', '']\n",
            "['', '', '', '', '', '', '', '', '', '', '']\n",
            "['', '', '', '', '', '', '', '', '', '', '']\n",
            "['', '', '', '', '', '', '', '', '', '', '']\n",
            "['', '', '', '', '', '', '', '', '', '', '']\n",
            "['', '', '', '', '', '', '', '', '', '', '']\n",
            "['', '', '', '', '', '', '', '', '', '', '']"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "with open(\"alpha-12-Sep-2022 21-28-50.csv\", \"r\") as file:\n",
        "  trans = csv.reader(file)\n",
        "  for line in trans:\n",
        "    print(line)"
      ],
      "id": "01cee34b-9746-444c-9254-b3dd74365091"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> This will be great if the idea is to process the file line (row) by\n",
        "> line.\n",
        "\n",
        "##### Read file using Pandas"
      ],
      "id": "a91413f8-e5a0-4b8c-abae-e656b86b9814"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"alpha-12-Sep-2022 21-28-50.csv\")"
      ],
      "id": "55736e4a-424b-4daf-9dd9-88ab00247c93"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "File can be read directly from github."
      ],
      "id": "5f6a1b3f-6d20-460f-a8c9-f03caa9ecb0c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/geshun/analytics-group/main/weeks/5/alpha-12-Sep-2022%2021-28-50.csv\")"
      ],
      "id": "7b32cc0f-a20f-48cb-a1b0-811f5120ae8a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Get Info and summary about the data\n",
        "\n",
        "The knowledge gained should be useful for processing the data for later\n",
        "analysis."
      ],
      "id": "11457414-d64c-4c47-89c1-d1773dbb0698"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(33, 11)"
          ]
        }
      ],
      "source": [
        "df.shape"
      ],
      "id": "9ddf1ed4-9e50-405c-9c2c-d8af3acadba2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> `df.shape` Gives a sense of how many records (rows) or fields\n",
        "> (columns) we are dealing with. Some data pre-processing can increase\n",
        "> (or decrease) the number of fields or decrease the number of records\n",
        "> (rows) - like dropping records with missing values."
      ],
      "id": "17d9238d-fa5b-41af-b906-7bb8f444fed4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 33 entries, 0 to 32\n",
            "Data columns (total 11 columns):\n",
            " #   Column                    Non-Null Count  Dtype  \n",
            "---  ------                    --------------  -----  \n",
            " 0   transaction_id            18 non-null     object \n",
            " 1   transaction_date          18 non-null     object \n",
            " 2   transaction_type          18 non-null     object \n",
            " 3   transaction_category      18 non-null     object \n",
            " 4   transaction_item          18 non-null     object \n",
            " 5   transaction_total_cost    18 non-null     object \n",
            " 6   transaction_quantity      10 non-null     float64\n",
            " 7   transaction_unit_price    18 non-null     object \n",
            " 8   transaction_source        18 non-null     object \n",
            " 9   transaction_measure_unit  10 non-null     object \n",
            " 10  transaction_remark        2 non-null      object \n",
            "dtypes: float64(1), object(10)\n",
            "memory usage: 3.0+ KB"
          ]
        }
      ],
      "source": [
        "df.info()"
      ],
      "id": "0969ce46-f96a-42ec-b1c3-a58f83682ba4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> `df.info()` Makes us aware of the data types of our columns. Here, you\n",
        "> should begin to find out if the data types produced by your\n",
        "> `pd.read_csv()` are what you intend to work with as they are or change\n",
        "> them. When the non-null count is not the same as the number of\n",
        "> entries, we conceive the idea that `pd.read_csv()` read in some\n",
        "> entries as `null`.\n",
        "\n",
        "The information provided by `df.info()` method, can also be established\n",
        "via the following:"
      ],
      "id": "b6bb0349-d4d1-4e35-b539-90c285d74227"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transaction_id               object\n",
            "transaction_date             object\n",
            "transaction_type             object\n",
            "transaction_category         object\n",
            "transaction_item             object\n",
            "transaction_total_cost       object\n",
            "transaction_quantity        float64\n",
            "transaction_unit_price       object\n",
            "transaction_source           object\n",
            "transaction_measure_unit     object\n",
            "transaction_remark           object\n",
            "dtype: object"
          ]
        }
      ],
      "source": [
        "df.dtypes"
      ],
      "id": "d9a79d68-dce3-4fb6-b73e-33ce084b4ff5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transaction_id              15\n",
            "transaction_date            15\n",
            "transaction_type            15\n",
            "transaction_category        15\n",
            "transaction_item            15\n",
            "transaction_total_cost      15\n",
            "transaction_quantity        23\n",
            "transaction_unit_price      15\n",
            "transaction_source          15\n",
            "transaction_measure_unit    23\n",
            "transaction_remark          31\n",
            "dtype: int64"
          ]
        }
      ],
      "source": [
        "df.isnull().sum()"
      ],
      "id": "e6fc6048-16e7-4d8a-b4d3-fcb40d4ae2a6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another piece of check is duplicates and it can be done using\n",
        "`duplicated()` method of a `DataFrame`. Can you try understanding what\n",
        "caused the duplicates in the data?"
      ],
      "id": "d2120a4c-b8f4-426c-93dc-dd07165c360e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14"
          ]
        }
      ],
      "source": [
        "df.duplicated().sum()"
      ],
      "id": "cb259c13-6cf3-4f07-a02a-c7c63a731e9b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Before you drop duplicates (de-duplicate) or remove missing values,\n",
        "> there is an important question one needs to address. What set of\n",
        "> fields identify a unique record in your data? This is the idea of\n",
        "> primary key in database terminology. In our case, only the\n",
        "> `transaction_id` field is the unique record identifier. This field\n",
        "> should always have a value (hence, can’t be null), however,\n",
        "> `df.isnull().sum()` shows that it has null entries."
      ],
      "id": "b64a31b5-a591-4ffe-8377-26333743379a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.dropna(subset=[\"transaction_id\"], inplace=True)"
      ],
      "id": "05182ae3-b5ca-4d57-8ff7-f10235c65ce4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Can you explain why we did not just do `df.dropna()`? Also, what is the\n",
        "significant of using `inplace=True`?*\n",
        "\n",
        "If we had wanted to check whether an entire row in our `DataFrame` has\n",
        "missing value, we could have done `df[df.isnull().all(axis=1)]`\n",
        "\n",
        "Go ahead and check that we have less records (`18`) than we started with\n",
        "(`33`). However, there are still missing values in our data."
      ],
      "id": "85d4406d-a2a9-4e8b-9fde-9edfb6cf1adf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transaction_id              18\n",
            "transaction_date             9\n",
            "transaction_type             4\n",
            "transaction_category        10\n",
            "transaction_item            10\n",
            "transaction_total_cost      10\n",
            "transaction_quantity         6\n",
            "transaction_unit_price       7\n",
            "transaction_source           8\n",
            "transaction_measure_unit     8\n",
            "transaction_remark           2\n",
            "dtype: int64"
          ]
        }
      ],
      "source": [
        "df.nunique()"
      ],
      "id": "15b12fad-8a76-46d3-9693-bf62d7c142ba"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> `df.nunique()` counts the number of unique entries in each column. One\n",
        "> application of this information is to establish if some fields can be\n",
        "> stored as `Category`. We normally convert an object field into a\n",
        "> category if there are fixed entries. The `transaction_type` field for\n",
        "> instance has no missing values and only `4` unique entries. You can\n",
        "> know the entries by doing `df.transaction_type.unique()`\n",
        "\n",
        "##### Create analytical dataset\n",
        "\n",
        "We shall take a column and ask if it is in the form we want to work\n",
        "with. Apart from the `transaction_quantity` column, all other was read\n",
        "in as `object` type (`Pandas`’ (specifically `numpy`’s) way of saying\n",
        "mixed data type and thus string."
      ],
      "id": "94378c75-e7ce-4e65-8df7-97903db767eb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[\"transaction_type\"] = df[\"transaction_type\"].astype(\"category\")\n",
        "df[\"transaction_type\"] = pd.Categorical(df[\"transaction_type\"])\n",
        "df = df.astype({\"transaction_type\": \"category\"})"
      ],
      "id": "0a347a6b-e6f1-4831-8ce7-ba7c3570d7ea"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Run any of the code above to cast the `transaction_type` column to\n",
        "> categorical.\n",
        "\n",
        "*In case we also wanted to cast the `transaction_category` column to\n",
        "categorical, how would we have done that together with\n",
        "`transaction_type`?*"
      ],
      "id": "d4c201e8-7bcd-4a5d-8d11-eeffc07b4196"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[\"transaction_total_cost\"].astype(\"float\")"
      ],
      "id": "4257b41d-1998-4cbc-96fd-bbaff12b7b59"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*The above code unsuccessfully attempts to cast the\n",
        "`transaction_total_cost` as float. Do you understand the error message\n",
        "produced? How will you deal with it?*"
      ],
      "id": "b596fa7e-40bd-447d-86c3-7fc4818266d4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[\"transaction_total_cost\"] = pd.to_numeric(df.transaction_total_cost.str.replace(\",\", \"\"))\n",
        "df[\"transaction_total_cost\"] = df[\"transaction_total_cost\"].str.replace(\",\", \"\").astype(\"float\")"
      ],
      "id": "ee34ae88-c140-4542-9d87-95b68a9f9f79"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Run any of the above code to cast `transaction_total_cost` as numeric.\n",
        "> Realize that, I had to remove the comma in the text prior to casting.\n",
        "> In `Pandas`, the `str` method grants us access to methods applicable\n",
        "> to strings, and from there we can use the `replace()` method. Here, we\n",
        "> are replacing comma `\",\"` with empty string - nothing `\"\"` when we do\n",
        "> `replace(\",\", \"\")`.\n",
        "\n",
        "If you want to look too exotic (which perhaps is unnecessary in this\n",
        "case), you can consider this:\n",
        "`df[\"transaction_total_cost\"] = df[\"transaction_total_cost\"].apply(lambda x: float(x.replace(\",\", \"\")))`.\n",
        "It uses `lambda` or anonymous function via the `apply` method of a\n",
        "`Series`."
      ],
      "id": "99b90aca-3a1c-4edf-8ea9-da3c6f2aae36"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[\"transaction_date\"] = df[\"transaction_date\"].astype(\"datetime64[ns]\")\n",
        "df[\"transaction_date\"] = pd.to_datetime(df[\"transaction_date\"])"
      ],
      "id": "5cf6d05d-8800-4f8d-b9ff-9436c7cf0f09"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Run any of the above code to get the `transaction_date` field as\n",
        "> datetime type.\n",
        "\n",
        "*We could have converted the `transaction_date` field together with\n",
        "`transaction_type`, can you work that out?*\n",
        "\n",
        "There is something interesting about the `transaction_unit_price`\n",
        "column. Try doing this and see if you will understand the error\n",
        "produced: `pd.to_numeric(df.transaction_unit_price)` or\n",
        "`df[\"transaction_unit_price\"].astype(float)`"
      ],
      "id": "b2273b17-87ec-4c30-b8b1-e4aa372ab472"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0      20.0\n",
            "1     120.0\n",
            "2     100.0\n",
            "3     100.0\n",
            "4      20.0\n",
            "5     200.0\n",
            "6     220.0\n",
            "7       NaN\n",
            "8       NaN\n",
            "9       NaN\n",
            "10      NaN\n",
            "11      NaN\n",
            "12      NaN\n",
            "13      NaN\n",
            "14      NaN\n",
            "15    100.0\n",
            "16      NaN\n",
            "17    500.0\n",
            "Name: transaction_unit_price, dtype: float64"
          ]
        }
      ],
      "source": [
        "df[\"transaction_unit_price\"].str.strip().apply(lambda x: float(x) if len(x) != 0 else np.nan)"
      ],
      "id": "6b2dbe2a-ae2b-460c-9dbd-88d2c3a88e18"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Run any of the above code to convert `transaction_unit_price` to\n",
        "> numeric."
      ],
      "id": "cc57ae37-56f6-47da-a2db-9759d15ad8d9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def to_float(x):\n",
        "  try:\n",
        "    return float(x)\n",
        "  except ValueError:\n",
        "    return np.nan"
      ],
      "id": "f80dec6e-36f9-4eeb-a717-b297ed50b71e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0      20.0\n",
            "1     120.0\n",
            "2     100.0\n",
            "3     100.0\n",
            "4      20.0\n",
            "5     200.0\n",
            "6     220.0\n",
            "7       NaN\n",
            "8       NaN\n",
            "9       NaN\n",
            "10      NaN\n",
            "11      NaN\n",
            "12      NaN\n",
            "13      NaN\n",
            "14      NaN\n",
            "15    100.0\n",
            "16      NaN\n",
            "17    500.0\n",
            "Name: transaction_unit_price, dtype: float64"
          ]
        }
      ],
      "source": [
        "df[\"transaction_unit_price\"].apply(to_float)"
      ],
      "id": "b0e7d7e6-faef-4ca0-a788-244abee593d2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also decided to write a function and apply it. We wrote a\n",
        "`to_float()` function and applied it. If you care about the list\n",
        "comprehension we talked about during the early days of our meetups see\n",
        "the immediate code below:"
      ],
      "id": "c5b2db75-4cad-4382-96f0-6ae92fddd57a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "[float(x.strip()) if x.strip().isnumeric() else np.nan for x in df.transaction_unit_price]"
      ],
      "id": "90b083e6-4d3f-408f-8074-f6fb3f30e166"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Compare your result with this: If after transforming your data, you get something similar, then things are getting interesting."
      ],
      "id": "dac4fb39-7d02-41c9-b4d4-e6712a25c934"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 18 entries, 0 to 17\n",
            "Data columns (total 11 columns):\n",
            " #   Column                    Non-Null Count  Dtype         \n",
            "---  ------                    --------------  -----         \n",
            " 0   transaction_id            18 non-null     object        \n",
            " 1   transaction_date          18 non-null     datetime64[ns]\n",
            " 2   transaction_type          18 non-null     category      \n",
            " 3   transaction_category      18 non-null     category      \n",
            " 4   transaction_item          18 non-null     object        \n",
            " 5   transaction_total_cost    18 non-null     float64       \n",
            " 6   transaction_quantity      10 non-null     float64       \n",
            " 7   transaction_unit_price    9 non-null      float64       \n",
            " 8   transaction_source        18 non-null     object        \n",
            " 9   transaction_measure_unit  10 non-null     object        \n",
            " 10  transaction_remark        2 non-null      object        \n",
            "dtypes: category(2), datetime64[ns](1), float64(3), object(5)\n",
            "memory usage: 2.0+ KB"
          ]
        }
      ],
      "source": [
        "from pandas import read_csv\n",
        "from numpy import nan\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/geshun/analytics-group/main/weeks/5/alpha-12-Sep-2022%2021-28-50.csv\"\n",
        "\n",
        "dt = read_csv(\n",
        "  url,\n",
        "  skipfooter=15,\n",
        "  engine=\"python\",\n",
        "  dtype={\n",
        "    \"transaction_type\": \"category\",\n",
        "    \"transaction_category\": \"category\"\n",
        "  },\n",
        "  converters={\n",
        "    \"transaction_unit_price\": lambda x: float(x) if len(x.strip()) != 0 else nan,\n",
        "    \"transaction_total_cost\": lambda x: float(x.replace(\",\", \"\")) if len(x.strip()) != 0 else nan\n",
        "  },\n",
        "  parse_dates=[\"transaction_date\"]\n",
        ")\n",
        "\n",
        "dt.info()"
      ],
      "id": "162e7bb9-0624-4082-b007-87f8cd153fa2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Save dataset"
      ],
      "id": "262792c4-82bc-484e-b14b-16df9d4cda59"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [],
      "id": "f01a5b9a-43e3-44ea-92ed-bff7fef4a46f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Gaining insight from data"
      ],
      "id": "9fb88180-ea82-47cb-939a-e93b65a07bfa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [],
      "id": "18db61c9-f206-4b60-806e-8512870abd9a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [],
      "id": "0f56f0fd-c358-4141-9f57-f86a07027e6a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [],
      "id": "97f95b9c-bde4-4974-8db4-bebe78b1ec1d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [],
      "id": "7ebbd80f-83c6-429d-9609-e8638af736b5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Import matplotlib"
      ],
      "id": "ab25e239-17c8-4699-8736-663b0a9ac216"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [],
      "id": "55b36bf6-9761-41e9-8a48-d754891a508c"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  }
}