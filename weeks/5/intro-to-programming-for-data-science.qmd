---
title: "Intro to Programming for Data Science"
subtitle: Practical Pandas
author: "geshun"
format: ipynb
editor: visual
---

```{r}
reticulate::use_condaenv("base")
```

##### Import pandas

Include `numpy` and `matplotlib`, we may use them down the line.

```{python}
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
```

##### Open and read files using vanilla python

> File name is `alpha-12-Sep-2022 21-28-50.csv`

```{python}
with open("alpha-12-Sep-2022 21-28-50.csv", "r") as f:
  transaction = f.read()

transaction
```

We see that a lot of work needs to be done on `transaction` string object if we want to get it into tabular/rectangular form or preprocess so that numeric fields are actually numeric.

> We can attempt to read the `.csv` file using the `csv` package which comes with standard python.

```{python}
import csv
with open("alpha-12-Sep-2022 21-28-50.csv", "r") as file:
  trans = csv.reader(file)
  for line in trans:
    print(line)
```

> This will be great if the idea is to process the file line (row) by line.

##### Read file using Pandas

```{python}
df = pd.read_csv("alpha-12-Sep-2022 21-28-50.csv")
```

File can be read directly from github.

```{python}
df = pd.read_csv("https://raw.githubusercontent.com/geshun/analytics-group/main/weeks/5/alpha-12-Sep-2022%2021-28-50.csv")
```

##### Get Info and summary about the data

The knowledge gained should be useful for processing the data for later analysis.

```{python}
df.shape
```

> `df.shape` Gives a sense of how many records (rows) or fields (columns) we are dealing with. Some data pre-processing can increase (or decrease) the number of fields or decrease the number of records (rows) - like dropping records with missing values.

```{python}
df.info()
```

> `df.info()` Makes us aware of the data types of our columns. Here, you should begin to find out if the data types produced by your `pd.read_csv()` are what you intend to work with as they are or change them. When the non-null count is not the same as the number of entries, we conceive the idea that `pd.read_csv()` read in some entries as `null`.

The information provided by `df.info()` method, can also be established via the following:

```{python}
df.dtypes
```

```{python}
df.isnull().sum()
```

Another piece of check is duplicates and it can be done using `duplicated()` method of a `DataFrame`. Can you try understanding what caused the duplicates in the data?

```{python}
df.duplicated().sum()
```

> Before you drop duplicates (de-duplicate) or remove missing values, there is an important question one needs to address. What set of fields identify a unique record in your data? This is the idea of primary key in database terminology. In our case, only the `transaction_id` field is the unique record identifier. This field should always have a value (hence, can't be null), however, `df.isnull().sum()` shows that it has null entries.

```{python}
df.dropna(subset=["transaction_id"], inplace=True)
# df = df.dropna(subset=["transaction_id"])
```

*Can you explain why we did not just do `df.dropna()`? Also, what is the significant of using `inplace=True`?*

If we had wanted to check whether an entire row in our `DataFrame` has missing value, we could have done `df[df.isnull().all(axis=1)]`

Go ahead and check that we have less records (`18`) than we started with (`33`). However, there are still missing values in our data.

```{python}
df.nunique()
```

> `df.nunique()` counts the number of unique entries in each column. One application of this information is to establish if some fields can be stored as `Category`. We normally convert an object field into a category if there are fixed entries. The `transaction_type` field for instance has no missing values and only `4` unique entries. You can know the entries by doing `df.transaction_type.unique()`

##### Create analytical dataset

We shall take a column and ask if it is in the form we want to work with. Apart from the `transaction_quantity` column, all other was read in as `object` type (`Pandas`' (specifically `numpy`'s) way of saying mixed data type and thus string.

```{python}
df["transaction_type"] = df["transaction_type"].astype("category")
df["transaction_type"] = pd.Categorical(df["transaction_type"])
df = df.astype({"transaction_type": "category"})
```

> Run any of the code above to cast the `transaction_type` column to categorical.

*In case we also wanted to cast the `transaction_category` column to categorical, how would we have done that together with `transaction_type`?*

```{python}
#| eval: false
df["transaction_total_cost"].astype("float")
```

*The above code unsuccessfully attempts to cast the `transaction_total_cost` as float. Do you understand the error message produced? How will you deal with it?*

```{python}
df["transaction_total_cost"] = pd.to_numeric(df.transaction_total_cost.str.replace(",", ""))
# df["transaction_total_cost"] = df["transaction_total_cost"].str.replace(",", "").astype(float)
```

> Run any of the above code to cast `transaction_total_cost` as numeric. Realize that, I had to remove the comma in the text prior to casting. In `Pandas`, the `str` method grants us access to methods applicable to strings, and from there we can use the `replace()` method. Here, we are replacing comma `","` with empty string - nothing `""` when we do `replace(",", "")`.

If you want to look too exotic (which perhaps is unnecessary in this case), you can consider this: `df["transaction_total_cost"] = df["transaction_total_cost"].apply(lambda x: float(x.replace(",", "")))`. It uses `lambda` or anonymous function via the `apply` method of a `Series`.

```{python}
df["transaction_date"] = df["transaction_date"].astype("datetime64[ns]")
df["transaction_date"] = pd.to_datetime(df["transaction_date"])
```

> Run any of the above code to get the `transaction_date` field as datetime type.

*We could have converted the `transaction_date` field together with `transaction_type`, can you work that out?*

There is something interesting about the `transaction_unit_price` column. Try doing this and see if you will understand the error produced: `pd.to_numeric(df.transaction_unit_price)` or `df["transaction_unit_price"].astype(float)`

```{python}
df["transaction_unit_price"] = df["transaction_unit_price"].str.strip().apply(lambda x: float(x) if len(x) != 0 else np.nan)
df["transaction_unit_price"] = pd.to_numeric(df.transaction_unit_price, errors='coerce')
```

> Run any of the above code to convert `transaction_unit_price` to numeric.

```{python}
def to_float(x):
  try:
    return float(x)
  except ValueError:
    return np.nan
```

```{python}
df["transaction_unit_price"].apply(to_float)
```

We can also decided to write a function and apply it. We wrote a `to_float()` function and applied it. If you care about the list comprehension we talked about during the early days of our meetups see the immediate code below:

```{python}
#| eval: false
[float(x.strip()) if x.strip().isnumeric() else np.nan for x in df.transaction_unit_price]
```

##### Compare your result with this: If after transforming your data, you get something similar, then things are getting interesting.

```{python}
from pandas import read_csv
from numpy import nan

url = "https://raw.githubusercontent.com/geshun/analytics-group/main/weeks/5/alpha-12-Sep-2022%2021-28-50.csv"

dt = read_csv(
  url,
  skipfooter=15,
  engine="python",
  dtype={
    "transaction_type": "category",
    "transaction_category": "category"
  },
  converters={
    "transaction_unit_price": lambda x: float(x) if len(x.strip()) != 0 else nan,
    "transaction_total_cost": lambda x: float(x.replace(",", "")) if len(x.strip()) != 0 else nan
  },
  parse_dates=["transaction_date"]
)

# dt.info()
```

##### Save dataset

```{python}
# df.to_csv("weeks/5/alpha.csv", index=False)
```

##### Gaining insight from data

First let's keep the columns simple and remove the redundant prefix name `transaction_`. We will also drop the last three columns (`source, measure_unit, remark`) we will not need them.

```{python}
df.columns
df.rename(columns=lambda col: col.replace("transaction_", ""), inplace=True)
df = df.iloc[:, :-3] # df.drop(["source", "measure_unit", "remark"], axis=1, inplace=True)
```

```{python}
pd.set_option("display.max_columns", 6)
pd.set_option("display.width", 150)
```

*How many Purchases and Sales have we made according to the dataset?*

-   This is a segmentation question (dealing with groups) - transactions that are related to purchases and those related to sales.

-   A classic split-apply-combine approach to problem solving

-   Basic count of elements belonging to each group

We ask the following data specific questions:

1.  What field captures information about purchases and sales?

```{python}
df["type"].value_counts()
df["type"].value_counts(normalize=True) * 100
```

```{python}
df["type"].value_counts().plot.barh()
```

*What if we want the count for just goods and services regardless of whether bought or sold?*

```{python}
df["type"].str.split(expand=True)[0].value_counts()
```

*How do we address the above question using `groupby` (which is a more general way of answering segmentation related questions)?*

```{python}
df.groupby(["type"]).size()
```

Exploring `DataFrameGroupBy` object

```{python}
m = df.groupby(["type"])
m.ngroups # how many groups
m.groups # group and the index of the rows that fall within it
list(m) # see data for each group
```

Essentially a `groupby` is having filtered data for each of the groups. `Groupby` just splits the data, however, we can apply some aggregation/summarization or function to the grouped data. Consider the question below:

*What is the total cost related to each transaction type?*

```{python}
df.groupby(["type"])["total_cost"].sum()
```

*Using the immediate result, establish if there is revenue gain or loss.*

```{python}
type_cost = df.groupby(["type"])["total_cost"].sum()
goods = type_cost["Sell Goods"] - type_cost["Buy Goods"]
services = type_cost["Sell Services"] - type_cost["Buy Services"]
goods + services
```

The result above is same as below. We have a loss since the cost is more than amount generated from the farm (thus negative revenue).

```{python}
type_cost.filter(like="Sell").sum() - type_cost.filter(like="Buy").sum()
```

If we want to use multiple summary function on a `groupby` object, we can do so via `agg()` method. The two code lines below produce the same result.

```{python}
df.groupby(["type"]).agg({"total_cost": ["mean", "sum"]})
df.groupby(["type"])["total_cost"].agg(["mean", "sum"])
```

*Find the record(s) related to most expensive transaction.*

```{python}
df[df.total_cost == df.total_cost.max()]
df[df.total_cost == df.total_cost.max()][["date", "item", "category", "total_cost"]]
```

*What type of transactions occurred in the month of April and what was the cost involved?*

This question gives us the chance to learn how to manipulate a date object in Pandas. In Pandas, a datetime object's properties and methods are exposed via the `dt` attribute.

```{python}
df[df.date.dt.month == 4].groupby("type")["total_cost"].sum()
```

*Establish if there is any particular day of the week where we tend to have many transactions.*

Realize that the above ask requires the day of the week, however, that information is not readily available in our data. We can create it. We will decide to turn our day of the week into an ordered categorical variable (you can do otherwise since ordering is not necessary to address the ask above). Take notice of how we used the `cat` attribute to get to the methods and properties of a category data type.

*Can you give a justification why an ordered day of the week is preferable?*

```{python}
days = ["Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"]

df = df.assign(wday = df.date.dt.day_name().astype("category"))
df = df.assign(wday = df.wday.cat.set_categories(days, ordered=True))

df.wday.value_counts(sort=False) 
df.groupby("wday").size() 
```

Though in our dataset and in the `wday` column, there is no entry with the value `Thursday` or `Saturday`, (***you can verify that with `df.wday.unique()`***) the count operation above, rightly indicated that there are no transactions on those days. If we had not turned the `wday` column into a categorical variable with explicit values, `days`, only the days that appear in the `wday` column would have have been displayed.

*Do you know why I had to set `sort=False` in `value_counts()`?*

##### Pivot Tables in Pandas

Start by reading the documentation `help(pd.pivot_table)` and see if you can make sense of the code below.

```{python}
pd.pivot_table(df, index="category", columns="type", values="total_cost", aggfunc="sum")
```
